{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dde8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawidwozniak/studies/sem9/computational_linguistic/lab01-language-model/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel, AutoModelForSequenceClassification\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from torchinfo import summary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff3fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"speakleash/Bielik-1.5B-v3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6849bf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = AutoModel.from_pretrained(model_name)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c8083b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "LlamaModel                                    --                        --\n",
       "├─Embedding: 1-1                              [1, 256, 1536]            49,152,000\n",
       "├─LlamaRotaryEmbedding: 1-2                   [1, 256, 128]             --\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─LlamaDecoderLayer: 2-1                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-1                 [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-2               [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-3                 [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-4                     [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-2                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-5                 [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-6               [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-7                 [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-8                     [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-3                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-9                 [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-10              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-11                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-12                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-4                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-13                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-14              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-15                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-16                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-5                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-17                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-18              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-19                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-20                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-6                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-21                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-22              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-23                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-24                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-7                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-25                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-26              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-27                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-28                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-8                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-29                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-30              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-31                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-32                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-9                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-33                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-34              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-35                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-36                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-10                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-37                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-38              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-39                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-40                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-11                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-41                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-42              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-43                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-44                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-12                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-45                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-46              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-47                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-48                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-13                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-49                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-50              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-51                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-52                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-14                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-53                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-54              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-55                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-56                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-15                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-57                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-58              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-59                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-60                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-16                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-61                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-62              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-63                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-64                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-17                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-65                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-66              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-67                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-68                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-18                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-69                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-70              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-71                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-72                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-19                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-73                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-74              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-75                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-76                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-20                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-77                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-78              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-79                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-80                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-21                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-81                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-82              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-83                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-84                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-22                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-85                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-86              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-87                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-88                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-23                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-89                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-90              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-91                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-92                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-24                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-93                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-94              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-95                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-96                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-25                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-97                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-98              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-99                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-100                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-26                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-101               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-102             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-103               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-104                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-27                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-105               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-106             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-107               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-108                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-28                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-109               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-110             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-111               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-112                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-29                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-113               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-114             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-115               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-116                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-30                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-117               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-118             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-119               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-120                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-31                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-121               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-122             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-123               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-124                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-32                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-125               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-126             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-127               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-128                   [1, 256, 1536]            41,307,136\n",
       "├─LlamaRMSNorm: 1-4                           [1, 256, 1536]            1,536\n",
       "===============================================================================================\n",
       "Total params: 1,547,355,648\n",
       "Trainable params: 1,547,355,648\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.55\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1717.57\n",
       "Params size (MB): 6189.42\n",
       "Estimated Total Size (MB): 7906.99\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 256))\n",
    "summary(model_2, input_data=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb0b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_2(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d908681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[ 1.6982, -3.0423,  0.2673,  ...,  0.5953, -1.4663, -0.2163],\n",
       "         [ 0.5338, -2.1484, -0.4599,  ..., -2.8242, -4.7383, -0.2091],\n",
       "         [ 1.3170, -0.1391, -1.6626,  ...,  0.5867,  1.2374, -0.4145],\n",
       "         ...,\n",
       "         [ 0.5909, -0.8897, -1.3753,  ..., -1.4780, -1.4284, -0.1146],\n",
       "         [ 0.3961, -0.8226, -1.4473,  ..., -0.8169, -0.7911,  0.1908],\n",
       "         [ 0.2619, -0.6083, -2.5031,  ..., -0.5375, -0.3433, -0.1376]]],\n",
       "       grad_fn=<MulBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6128e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256, 1536]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape, output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a1aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in model_2.parameters():\n",
    "    elem.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c49bff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "LlamaModel                                    --                        --\n",
       "├─Embedding: 1-1                              [1, 256, 1536]            (49,152,000)\n",
       "├─LlamaRotaryEmbedding: 1-2                   [1, 256, 128]             --\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─LlamaDecoderLayer: 2-1                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-1                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-2               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-3                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-4                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-2                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-5                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-6               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-7                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-8                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-3                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-9                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-10              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-11                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-12                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-4                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-13                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-14              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-15                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-16                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-5                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-17                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-18              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-19                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-20                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-6                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-21                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-22              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-23                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-24                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-7                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-25                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-26              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-27                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-28                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-8                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-29                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-30              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-31                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-32                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-9                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-33                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-34              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-35                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-36                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-10                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-37                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-38              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-39                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-40                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-11                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-41                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-42              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-43                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-44                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-12                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-45                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-46              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-47                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-48                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-13                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-49                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-50              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-51                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-52                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-14                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-53                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-54              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-55                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-56                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-15                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-57                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-58              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-59                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-60                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-16                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-61                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-62              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-63                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-64                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-17                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-65                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-66              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-67                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-68                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-18                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-69                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-70              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-71                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-72                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-19                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-73                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-74              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-75                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-76                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-20                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-77                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-78              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-79                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-80                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-21                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-81                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-82              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-83                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-84                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-22                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-85                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-86              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-87                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-88                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-23                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-89                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-90              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-91                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-92                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-24                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-93                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-94              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-95                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-96                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-25                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-97                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-98              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-99                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-100                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-26                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-101               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-102             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-103               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-104                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-27                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-105               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-106             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-107               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-108                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-28                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-109               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-110             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-111               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-112                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-29                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-113               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-114             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-115               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-116                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-30                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-117               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-118             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-119               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-120                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-31                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-121               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-122             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-123               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-124                   [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-32                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-125               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-126             [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-127               [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-128                   [1, 256, 1536]            (41,307,136)\n",
       "├─LlamaRMSNorm: 1-4                           [1, 256, 1536]            (1,536)\n",
       "===============================================================================================\n",
       "Total params: 1,547,355,648\n",
       "Trainable params: 0\n",
       "Non-trainable params: 1,547,355,648\n",
       "Total mult-adds (Units.GIGABYTES): 1.55\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1717.57\n",
       "Params size (MB): 6189.42\n",
       "Estimated Total Size (MB): 7906.99\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 256))\n",
    "summary(model_2, input_data=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dca00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in model_2.layers[-15:]:\n",
    "    for param in elem.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca31df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "LlamaModel                                    --                        --\n",
       "├─Embedding: 1-1                              [1, 256, 1536]            (49,152,000)\n",
       "├─LlamaRotaryEmbedding: 1-2                   [1, 256, 128]             --\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─LlamaDecoderLayer: 2-1                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-1                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-2               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-3                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-4                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-2                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-5                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-6               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-7                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-8                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-3                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-9                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-10              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-11                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-12                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-4                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-13                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-14              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-15                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-16                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-5                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-17                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-18              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-19                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-20                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-6                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-21                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-22              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-23                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-24                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-7                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-25                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-26              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-27                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-28                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-8                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-29                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-30              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-31                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-32                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-9                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-33                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-34              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-35                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-36                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-10                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-37                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-38              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-39                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-40                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-11                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-41                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-42              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-43                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-44                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-12                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-45                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-46              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-47                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-48                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-13                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-49                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-50              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-51                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-52                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-14                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-53                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-54              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-55                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-56                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-15                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-57                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-58              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-59                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-60                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-16                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-61                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-62              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-63                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-64                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-17                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-65                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-66              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-67                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-68                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-18                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-69                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-70              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-71                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-72                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-19                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-73                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-74              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-75                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-76                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-20                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-77                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-78              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-79                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-80                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-21                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-81                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-82              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-83                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-84                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-22                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-85                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-86              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-87                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-88                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-23                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-89                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-90              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-91                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-92                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-24                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-93                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-94              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-95                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-96                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-25                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-97                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-98              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-99                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-100                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-26                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-101               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-102             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-103               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-104                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-27                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-105               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-106             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-107               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-108                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-28                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-109               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-110             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-111               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-112                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-29                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-113               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-114             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-115               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-116                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-30                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-117               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-118             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-119               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-120                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-31                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-121               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-122             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-123               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-124                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-32                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-125               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-126             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-127               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-128                   [1, 256, 1536]            41,307,136\n",
       "├─LlamaRMSNorm: 1-4                           [1, 256, 1536]            (1,536)\n",
       "===============================================================================================\n",
       "Total params: 1,547,355,648\n",
       "Trainable params: 702,282,240\n",
       "Non-trainable params: 845,073,408\n",
       "Total mult-adds (Units.GIGABYTES): 1.55\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1717.57\n",
       "Params size (MB): 6189.42\n",
       "Estimated Total Size (MB): 7906.99\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 256))\n",
    "summary(model_2, input_data=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13d0258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ceffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.norm.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df5aac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "LlamaModel                                    --                        --\n",
       "├─Embedding: 1-1                              [1, 256, 1536]            (49,152,000)\n",
       "├─LlamaRotaryEmbedding: 1-2                   [1, 256, 128]             --\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─LlamaDecoderLayer: 2-1                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-1                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-2               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-3                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-4                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-2                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-5                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-6               [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-7                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-8                     [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-3                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-9                 [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-10              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-11                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-12                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-4                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-13                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-14              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-15                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-16                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-5                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-17                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-18              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-19                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-20                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-6                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-21                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-22              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-23                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-24                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-7                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-25                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-26              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-27                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-28                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-8                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-29                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-30              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-31                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-32                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-9                 [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-33                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-34              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-35                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-36                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-10                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-37                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-38              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-39                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-40                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-11                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-41                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-42              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-43                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-44                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-12                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-45                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-46              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-47                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-48                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-13                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-49                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-50              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-51                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-52                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-14                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-53                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-54              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-55                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-56                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-15                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-57                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-58              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-59                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-60                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-16                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-61                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-62              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-63                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-64                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-17                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-65                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaAttention: 3-66              [1, 256, 1536]            (5,508,608)\n",
       "│    │    └─LlamaRMSNorm: 3-67                [1, 256, 1536]            (1,536)\n",
       "│    │    └─LlamaMLP: 3-68                    [1, 256, 1536]            (41,307,136)\n",
       "│    └─LlamaDecoderLayer: 2-18                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-69                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-70              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-71                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-72                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-19                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-73                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-74              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-75                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-76                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-20                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-77                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-78              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-79                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-80                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-21                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-81                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-82              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-83                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-84                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-22                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-85                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-86              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-87                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-88                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-23                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-89                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-90              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-91                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-92                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-24                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-93                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-94              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-95                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-96                    [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-25                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-97                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-98              [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-99                [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-100                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-26                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-101               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-102             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-103               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-104                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-27                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-105               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-106             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-107               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-108                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-28                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-109               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-110             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-111               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-112                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-29                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-113               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-114             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-115               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-116                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-30                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-117               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-118             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-119               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-120                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-31                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-121               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-122             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-123               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-124                   [1, 256, 1536]            41,307,136\n",
       "│    └─LlamaDecoderLayer: 2-32                [1, 256, 1536]            --\n",
       "│    │    └─LlamaRMSNorm: 3-125               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaAttention: 3-126             [1, 256, 1536]            5,508,608\n",
       "│    │    └─LlamaRMSNorm: 3-127               [1, 256, 1536]            1,536\n",
       "│    │    └─LlamaMLP: 3-128                   [1, 256, 1536]            41,307,136\n",
       "├─LlamaRMSNorm: 1-4                           [1, 256, 1536]            (1,536)\n",
       "===============================================================================================\n",
       "Total params: 1,547,355,648\n",
       "Trainable params: 702,282,240\n",
       "Non-trainable params: 845,073,408\n",
       "Total mult-adds (Units.GIGABYTES): 1.55\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1717.57\n",
       "Params size (MB): 6189.42\n",
       "Estimated Total Size (MB): 7906.99\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 256))\n",
    "summary(model_2, input_data=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25ebe165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "163514ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d11fcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (up_proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "        (down_proj): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88fcf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([2.6406, 2.9375, 2.9219,  ..., 3.0000, 3.0312, 3.0938])\n"
     ]
    }
   ],
   "source": [
    "for param in model_2.norm.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90c1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab01-language-model (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
