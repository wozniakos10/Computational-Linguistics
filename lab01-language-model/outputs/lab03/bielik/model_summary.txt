Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
CustomDecoderClassifier                            [1, 3]                    --
├─LlamaModel: 1-1                                  --                        --
│    └─Embedding: 2-1                              [1, 256, 1536]            (49,152,000)
│    └─LlamaRotaryEmbedding: 2-2                   [1, 256, 128]             --
│    └─ModuleList: 2-3                             --                        --
│    │    └─LlamaDecoderLayer: 3-1                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-2                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-3                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-4                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-5                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-6                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-7                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-8                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-9                 [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-10                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-11                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-12                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-13                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-14                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-15                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-16                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-17                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-18                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-19                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-20                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-21                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-22                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-23                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-24                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-25                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-26                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-27                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-28                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-29                [1, 256, 1536]            (46,818,816)
│    │    └─LlamaDecoderLayer: 3-30                [1, 256, 1536]            46,818,816
│    │    └─LlamaDecoderLayer: 3-31                [1, 256, 1536]            46,818,816
│    │    └─LlamaDecoderLayer: 3-32                [1, 256, 1536]            46,818,816
│    └─LlamaRMSNorm: 2-4                           [1, 256, 1536]            1,536
├─Sequential: 1-2                                  [1, 3]                    --
│    └─Linear: 2-5                                 [1, 512]                  786,944
│    └─LayerNorm: 2-6                              [1, 512]                  1,024
│    └─GELU: 2-7                                   [1, 512]                  --
│    └─Dropout: 2-8                                [1, 512]                  --
│    └─Linear: 2-9                                 [1, 3]                    1,539
====================================================================================================
Total params: 1,548,145,155
Trainable params: 141,247,491
Non-trainable params: 1,406,897,664
Total mult-adds (Units.GIGABYTES): 1.55
====================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 1717.58
Params size (MB): 6192.58
Estimated Total Size (MB): 7910.16
====================================================================================================